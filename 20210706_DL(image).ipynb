{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e91bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import struct, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03bc534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind = 'train'):\n",
    "    labels_path = os.path.join(path, \"%s-labels-idx1-ubyte\"%kind)\n",
    "    images_path = os.path.join(path, \"%s-images-idx3-ubyte\"%kind)\n",
    "    # label\n",
    "    with open(labels_path, 'rb') as la_path:\n",
    "        magic, n = struct.unpack(\">II\", la_path.read(8))\n",
    "        labels = np.fromfile(la_path, dtype = np.uint8)\n",
    "    # image\n",
    "    with open(images_path, 'rb') as img_path:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", img_path.read(16))\n",
    "        images = np.fromfile(img_path, dtype = np.uint8).reshape(len(labels), 28**2)\n",
    "        images = ((images / 255) - 0.5) * 2\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5360cddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 784\n",
      "10000 784\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist(\"./\", kind = 'train')\n",
    "X_test, y_test = load_mnist(\"./\", kind = 't10k')\n",
    "print(X_train.shape[0], X_train.shape[1])\n",
    "print(X_test.shape[0], X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0960b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = np.mean(X_train, axis = 0)\n",
    "std_val = np.std(X_train)\n",
    "X_train_centered = (X_train - mean_vals) / std_val\n",
    "X_test_centered = (X_test - mean_vals) / std_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02f7f074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_onehot = tf.keras.utils.to_categorical(y_train)\n",
    "y_train_onehot[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23639e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units = 50, \n",
    "    input_dim = X_train_centered.shape[1], \n",
    "    kernel_initializer = 'glorot_uniform', \n",
    "    bias_initializer = 'zeros', \n",
    "    activation = 'tanh'\n",
    "))\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units = 50, \n",
    "    input_dim = 50, \n",
    "    kernel_initializer = 'glorot_uniform', \n",
    "    bias_initializer = 'zeros', \n",
    "    activation = 'tanh'\n",
    "))\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units = 10, \n",
    "    input_dim = 50, \n",
    "    kernel_initializer = 'glorot_uniform', \n",
    "    bias_initializer = 'zeros', \n",
    "    activation = 'softmax'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce6375ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 42,310\n",
      "Trainable params: 42,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e548439",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = tf.keras.optimizers.SGD(lr = 0.0001, decay = 1e-7, momentum = 0.9)\n",
    "model.compile(optimizer = sgd_optimizer, loss = \"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d64c8806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 1.6270 - val_loss: 1.1053\n",
      "Epoch 2/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.9962 - val_loss: 0.7960\n",
      "Epoch 3/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.7879 - val_loss: 0.6483\n",
      "Epoch 4/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.6736 - val_loss: 0.5599\n",
      "Epoch 5/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.5999 - val_loss: 0.5005\n",
      "Epoch 6/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.5478 - val_loss: 0.4579\n",
      "Epoch 7/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.5088 - val_loss: 0.4257\n",
      "Epoch 8/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.4783 - val_loss: 0.4001\n",
      "Epoch 9/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.4537 - val_loss: 0.3794\n",
      "Epoch 10/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.4335 - val_loss: 0.3623\n",
      "Epoch 11/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.4163 - val_loss: 0.3479\n",
      "Epoch 12/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.4016 - val_loss: 0.3353\n",
      "Epoch 13/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3887 - val_loss: 0.3245\n",
      "Epoch 14/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3774 - val_loss: 0.3148\n",
      "Epoch 15/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3673 - val_loss: 0.3063\n",
      "Epoch 16/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3582 - val_loss: 0.2986\n",
      "Epoch 17/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3500 - val_loss: 0.2916\n",
      "Epoch 18/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3424 - val_loss: 0.2852\n",
      "Epoch 19/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3355 - val_loss: 0.2793\n",
      "Epoch 20/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3291 - val_loss: 0.2740\n",
      "Epoch 21/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3231 - val_loss: 0.2690\n",
      "Epoch 22/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3176 - val_loss: 0.2643\n",
      "Epoch 23/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3123 - val_loss: 0.2601\n",
      "Epoch 24/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3074 - val_loss: 0.2560\n",
      "Epoch 25/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3028 - val_loss: 0.2522\n",
      "Epoch 26/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2984 - val_loss: 0.2486\n",
      "Epoch 27/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2943 - val_loss: 0.2453\n",
      "Epoch 28/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2903 - val_loss: 0.2420\n",
      "Epoch 29/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2865 - val_loss: 0.2391\n",
      "Epoch 30/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2829 - val_loss: 0.2362\n",
      "Epoch 31/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2795 - val_loss: 0.2334\n",
      "Epoch 32/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2762 - val_loss: 0.2307\n",
      "Epoch 33/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2730 - val_loss: 0.2283\n",
      "Epoch 34/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2699 - val_loss: 0.2258\n",
      "Epoch 35/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2669 - val_loss: 0.2235\n",
      "Epoch 36/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2641 - val_loss: 0.2213\n",
      "Epoch 37/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2613 - val_loss: 0.2192\n",
      "Epoch 38/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2587 - val_loss: 0.2172\n",
      "Epoch 39/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2561 - val_loss: 0.2152\n",
      "Epoch 40/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2536 - val_loss: 0.2132\n",
      "Epoch 41/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2511 - val_loss: 0.2114\n",
      "Epoch 42/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2488 - val_loss: 0.2096\n",
      "Epoch 43/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2464 - val_loss: 0.2078\n",
      "Epoch 44/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2442 - val_loss: 0.2062\n",
      "Epoch 45/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2420 - val_loss: 0.2045\n",
      "Epoch 46/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2399 - val_loss: 0.2029\n",
      "Epoch 47/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2378 - val_loss: 0.2014\n",
      "Epoch 48/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2357 - val_loss: 0.1999\n",
      "Epoch 49/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2337 - val_loss: 0.1985\n",
      "Epoch 50/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2318 - val_loss: 0.1970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20d85dea790>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_centered, y_train_onehot, batch_size = 64, epochs = 50, verbose = 1, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cd9ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose = 0)\n",
    "y_train_pred[:3]\n",
    "y_test_pred = model.predict_classes(X_test_centered, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "478b9769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4], dtype=uint8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d2d80c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9320\n",
      "0.932\n"
     ]
    }
   ],
   "source": [
    "total_predicts = np.sum(y_test == y_test_pred, axis = 0)\n",
    "print(total_predicts)\n",
    "\n",
    "test_res = total_predicts / y_test.shape[0]\n",
    "print(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9e95789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 5 6\n",
      "33 4 6\n",
      "66 6 7\n",
      "92 9 4\n",
      "124 7 4\n",
      "149 2 9\n",
      "193 9 3\n",
      "195 3 5\n",
      "217 6 5\n",
      "233 8 7\n",
      "241 9 8\n",
      "245 3 5\n",
      "247 4 6\n",
      "259 6 0\n",
      "290 8 4\n",
      "313 3 5\n",
      "320 9 7\n",
      "321 2 7\n",
      "340 5 3\n",
      "341 6 4\n",
      "352 5 0\n",
      "358 7 9\n",
      "359 9 4\n",
      "380 0 5\n",
      "381 3 7\n",
      "403 8 5\n",
      "412 5 3\n",
      "444 2 8\n",
      "445 6 0\n",
      "448 9 8\n",
      "449 3 5\n",
      "478 5 8\n",
      "479 9 3\n",
      "495 8 0\n",
      "502 5 3\n",
      "507 3 5\n",
      "511 4 8\n",
      "530 9 4\n",
      "531 3 6\n",
      "543 8 3\n",
      "551 7 1\n",
      "565 4 9\n",
      "578 3 2\n",
      "582 8 2\n",
      "591 8 3\n",
      "619 1 8\n",
      "628 3 9\n",
      "629 2 6\n",
      "659 2 9\n",
      "684 7 3\n",
      "691 8 4\n",
      "707 4 9\n",
      "717 0 6\n",
      "720 5 8\n",
      "728 2 8\n",
      "740 4 9\n",
      "760 4 9\n",
      "781 8 5\n",
      "791 5 9\n",
      "839 8 3\n",
      "844 8 7\n",
      "857 5 3\n",
      "874 9 4\n",
      "882 9 7\n",
      "898 7 2\n",
      "924 2 7\n",
      "938 3 5\n",
      "939 2 0\n",
      "947 8 9\n",
      "950 7 2\n",
      "956 1 3\n",
      "959 4 9\n",
      "965 6 0\n",
      "999 9 7\n",
      "1003 5 3\n",
      "1014 6 5\n",
      "1032 5 2\n",
      "1039 7 9\n",
      "1044 6 2\n",
      "1050 2 6\n",
      "1062 3 7\n",
      "1068 8 4\n",
      "1073 5 8\n",
      "1082 5 3\n",
      "1092 3 2\n",
      "1101 8 2\n",
      "1107 9 3\n",
      "1112 4 6\n",
      "1114 3 8\n",
      "1119 7 2\n",
      "1124 8 7\n",
      "1125 8 9\n",
      "1128 3 7\n",
      "1131 5 4\n",
      "1173 7 9\n",
      "1181 6 1\n",
      "1182 6 8\n",
      "1185 8 9\n",
      "1191 0 4\n",
      "1192 9 4\n",
      "1194 7 9\n",
      "1198 8 4\n",
      "1200 8 5\n",
      "1202 8 5\n",
      "1204 3 2\n",
      "1206 7 2\n",
      "1208 3 9\n",
      "1226 7 2\n",
      "1232 9 4\n",
      "1234 8 5\n",
      "1242 4 9\n",
      "1243 5 6\n",
      "1247 9 0\n",
      "1256 2 3\n",
      "1260 7 1\n",
      "1283 7 2\n",
      "1289 5 9\n",
      "1299 5 7\n",
      "1319 8 3\n",
      "1326 7 2\n",
      "1328 7 9\n",
      "1337 2 6\n",
      "1364 8 6\n",
      "1378 5 8\n",
      "1393 5 3\n",
      "1410 2 6\n",
      "1433 8 1\n",
      "1453 4 9\n",
      "1466 5 3\n",
      "1500 7 1\n",
      "1522 7 9\n",
      "1525 5 0\n",
      "1527 1 6\n",
      "1530 8 7\n",
      "1549 4 6\n",
      "1553 9 8\n",
      "1559 9 3\n",
      "1569 6 4\n",
      "1581 7 9\n",
      "1609 2 6\n",
      "1621 0 6\n",
      "1634 4 7\n",
      "1640 9 7\n",
      "1671 7 3\n",
      "1678 2 0\n",
      "1681 3 7\n",
      "1686 8 5\n",
      "1709 9 5\n",
      "1717 8 0\n",
      "1718 7 3\n",
      "1722 2 4\n",
      "1737 5 2\n",
      "1754 7 2\n",
      "1759 8 6\n",
      "1772 7 9\n",
      "1774 8 5\n",
      "1782 8 4\n",
      "1790 2 9\n",
      "1800 6 4\n",
      "1813 8 5\n",
      "1823 8 9\n",
      "1828 3 7\n",
      "1850 8 9\n",
      "1855 8 2\n",
      "1878 8 3\n",
      "1883 7 9\n",
      "1901 9 4\n",
      "1917 5 8\n",
      "1926 3 5\n",
      "1938 4 6\n",
      "1941 7 8\n",
      "1942 8 4\n",
      "1952 9 3\n",
      "1970 5 3\n",
      "1973 8 5\n",
      "1981 6 4\n",
      "1984 2 0\n",
      "2004 8 9\n",
      "2016 7 2\n",
      "2024 7 9\n",
      "2035 5 3\n",
      "2040 5 4\n",
      "2043 4 8\n",
      "2044 2 7\n",
      "2052 8 4\n",
      "2053 4 9\n",
      "2068 9 4\n",
      "2070 7 4\n",
      "2093 8 1\n",
      "2098 2 0\n",
      "2099 8 9\n",
      "2109 3 4\n",
      "2118 6 0\n",
      "2129 9 2\n",
      "2130 4 9\n",
      "2131 6 2\n",
      "2135 6 1\n",
      "2138 2 8\n",
      "2148 4 9\n",
      "2182 1 3\n",
      "2185 0 5\n",
      "2186 2 5\n",
      "2189 9 1\n",
      "2224 5 6\n",
      "2266 1 6\n",
      "2272 8 0\n",
      "2293 9 0\n",
      "2299 2 8\n",
      "2305 3 8\n",
      "2312 3 5\n",
      "2325 7 2\n",
      "2369 5 9\n",
      "2370 0 6\n",
      "2371 4 9\n",
      "2378 0 2\n",
      "2380 9 0\n",
      "2393 8 5\n",
      "2394 4 9\n",
      "2395 8 3\n",
      "2406 9 1\n",
      "2408 3 9\n",
      "2414 9 4\n",
      "2422 6 4\n",
      "2425 8 3\n",
      "2460 5 8\n",
      "2488 2 4\n",
      "2526 5 3\n",
      "2534 3 5\n",
      "2545 5 3\n",
      "2556 5 8\n",
      "2573 5 8\n",
      "2574 5 7\n",
      "2586 5 3\n",
      "2598 8 2\n",
      "2607 7 8\n",
      "2610 2 8\n",
      "2617 8 7\n",
      "2635 2 8\n",
      "2648 9 0\n",
      "2654 6 1\n",
      "2668 5 6\n",
      "2670 5 8\n",
      "2684 3 7\n",
      "2695 7 4\n",
      "2698 5 3\n",
      "2713 0 8\n",
      "2720 9 4\n",
      "2730 7 4\n",
      "2760 9 4\n",
      "2770 3 6\n",
      "2771 4 9\n",
      "2780 2 3\n",
      "2810 5 3\n",
      "2832 5 3\n",
      "2836 4 9\n",
      "2850 5 3\n",
      "2863 9 4\n",
      "2896 8 0\n",
      "2906 3 5\n",
      "2907 4 9\n",
      "2915 7 3\n",
      "2925 5 0\n",
      "2927 3 2\n",
      "2945 3 7\n",
      "2953 3 5\n",
      "2990 8 9\n",
      "2995 6 8\n",
      "3005 9 1\n",
      "3060 9 7\n",
      "3073 1 2\n",
      "3102 5 3\n",
      "3110 3 5\n",
      "3114 4 6\n",
      "3117 5 9\n",
      "3130 6 0\n",
      "3136 7 9\n",
      "3145 5 9\n",
      "3166 7 9\n",
      "3167 3 1\n",
      "3186 8 2\n",
      "3189 7 9\n",
      "3206 8 3\n",
      "3269 6 2\n",
      "3280 2 8\n",
      "3284 8 7\n",
      "3289 8 9\n",
      "3329 7 2\n",
      "3330 2 3\n",
      "3376 7 9\n",
      "3405 4 9\n",
      "3447 6 4\n",
      "3448 3 5\n",
      "3450 0 8\n",
      "3475 3 7\n",
      "3490 4 9\n",
      "3503 9 1\n",
      "3520 6 4\n",
      "3525 7 2\n",
      "3549 3 2\n",
      "3550 6 5\n",
      "3558 5 0\n",
      "3559 8 5\n",
      "3565 5 8\n",
      "3567 8 5\n",
      "3573 7 4\n",
      "3575 7 8\n",
      "3580 7 1\n",
      "3597 9 3\n",
      "3598 1 8\n",
      "3604 7 0\n",
      "3629 8 3\n",
      "3667 7 9\n",
      "3674 8 5\n",
      "3681 2 3\n",
      "3702 5 3\n",
      "3716 9 3\n",
      "3718 4 9\n",
      "3726 4 9\n",
      "3730 7 9\n",
      "3751 7 1\n",
      "3757 8 1\n",
      "3763 5 4\n",
      "3767 7 2\n",
      "3776 5 8\n",
      "3780 4 6\n",
      "3806 5 8\n",
      "3808 7 3\n",
      "3811 2 3\n",
      "3817 2 6\n",
      "3818 0 6\n",
      "3820 9 7\n",
      "3821 9 4\n",
      "3833 8 1\n",
      "3836 7 9\n",
      "3838 7 1\n",
      "3846 6 2\n",
      "3848 7 3\n",
      "3853 6 5\n",
      "3855 5 0\n",
      "3869 9 4\n",
      "3876 2 8\n",
      "3893 5 6\n",
      "3902 5 3\n",
      "3906 1 3\n",
      "3926 9 3\n",
      "3941 4 6\n",
      "3943 3 5\n",
      "3946 2 8\n",
      "3951 8 7\n",
      "3962 3 6\n",
      "3968 5 3\n",
      "3976 7 1\n",
      "3984 9 8\n",
      "3985 9 4\n",
      "4000 9 4\n",
      "4017 4 9\n",
      "4027 7 9\n",
      "4044 3 5\n",
      "4063 6 5\n",
      "4065 0 6\n",
      "4072 5 3\n",
      "4075 8 5\n",
      "4078 9 7\n",
      "4093 9 4\n",
      "4131 5 3\n",
      "4140 8 3\n",
      "4152 5 1\n",
      "4154 9 4\n",
      "4163 9 5\n",
      "4176 2 6\n",
      "4177 5 4\n",
      "4199 7 9\n",
      "4201 1 7\n",
      "4205 2 6\n",
      "4211 6 5\n",
      "4212 1 3\n",
      "4224 9 7\n",
      "4238 7 9\n",
      "4248 2 1\n",
      "4256 3 2\n",
      "4271 5 3\n",
      "4286 0 2\n",
      "4289 2 7\n",
      "4297 7 3\n",
      "4300 5 8\n",
      "4302 5 8\n",
      "4306 3 7\n",
      "4313 4 9\n",
      "4317 3 9\n",
      "4330 5 8\n",
      "4341 2 3\n",
      "4355 5 9\n",
      "4356 5 8\n",
      "4369 9 4\n",
      "4374 5 6\n",
      "4382 4 9\n",
      "4400 7 9\n",
      "4405 9 4\n",
      "4425 9 4\n",
      "4427 2 8\n",
      "4433 7 3\n",
      "4435 3 7\n",
      "4449 6 0\n",
      "4451 2 7\n",
      "4463 5 6\n",
      "4477 0 6\n",
      "4497 8 7\n",
      "4498 7 9\n",
      "4523 8 3\n",
      "4534 9 8\n",
      "4540 7 9\n",
      "4548 5 2\n",
      "4571 6 2\n",
      "4575 4 2\n",
      "4578 7 9\n",
      "4601 8 4\n",
      "4615 2 4\n",
      "4630 3 5\n",
      "4639 8 3\n",
      "4690 7 2\n",
      "4724 8 0\n",
      "4731 8 7\n",
      "4735 9 4\n",
      "4740 3 5\n",
      "4751 4 6\n",
      "4761 9 1\n",
      "4785 3 8\n",
      "4807 8 0\n",
      "4808 3 5\n",
      "4814 6 0\n",
      "4823 9 4\n",
      "4826 4 9\n",
      "4829 8 3\n",
      "4837 7 2\n",
      "4852 8 5\n",
      "4874 9 0\n",
      "4876 2 4\n",
      "4879 8 4\n",
      "4880 0 8\n",
      "4886 7 1\n",
      "4888 5 0\n",
      "4890 8 6\n",
      "4915 5 8\n",
      "4950 2 3\n",
      "4956 8 4\n",
      "4966 7 1\n",
      "4990 3 2\n",
      "4995 2 3\n",
      "5001 9 4\n",
      "5015 9 4\n",
      "5054 3 5\n",
      "5078 3 2\n",
      "5140 3 4\n",
      "5183 8 4\n",
      "5210 9 7\n",
      "5331 1 6\n",
      "5457 1 8\n",
      "5523 9 7\n",
      "5569 8 3\n",
      "5600 7 9\n",
      "5601 8 4\n",
      "5608 5 6\n",
      "5611 8 1\n",
      "5617 4 9\n",
      "5620 7 9\n",
      "5642 1 5\n",
      "5676 4 7\n",
      "5691 4 1\n",
      "5714 7 9\n",
      "5717 2 8\n",
      "5718 0 8\n",
      "5734 3 7\n",
      "5749 8 6\n",
      "5835 7 9\n",
      "5842 4 7\n",
      "5862 5 3\n",
      "5867 5 3\n",
      "5874 5 3\n",
      "5887 7 2\n",
      "5888 4 0\n",
      "5891 5 3\n",
      "5912 3 0\n",
      "5913 5 3\n",
      "5922 5 3\n",
      "5935 3 8\n",
      "5936 4 9\n",
      "5937 5 3\n",
      "5955 3 8\n",
      "5972 5 3\n",
      "5973 3 8\n",
      "5985 5 8\n",
      "6019 4 9\n",
      "6023 3 5\n",
      "6026 7 1\n",
      "6035 2 0\n",
      "6042 5 8\n",
      "6043 5 3\n",
      "6045 3 9\n",
      "6059 3 9\n",
      "6065 3 8\n",
      "6071 9 3\n",
      "6081 9 5\n",
      "6091 9 0\n",
      "6157 9 0\n",
      "6166 9 3\n",
      "6168 9 3\n",
      "6172 9 0\n",
      "6173 9 0\n",
      "6227 5 3\n",
      "6324 5 6\n",
      "6347 8 6\n",
      "6391 2 6\n",
      "6400 0 6\n",
      "6421 3 2\n",
      "6425 6 2\n",
      "6426 0 6\n",
      "6428 0 6\n",
      "6480 2 6\n",
      "6505 9 0\n",
      "6555 8 7\n",
      "6568 9 4\n",
      "6569 3 9\n",
      "6571 9 7\n",
      "6590 0 5\n",
      "6597 0 7\n",
      "6598 5 6\n",
      "6603 8 7\n",
      "6613 6 4\n",
      "6632 9 5\n",
      "6641 8 5\n",
      "6642 9 4\n",
      "6651 0 5\n",
      "6688 1 8\n",
      "6706 5 7\n",
      "6721 2 4\n",
      "6740 9 0\n",
      "6744 2 8\n",
      "6746 5 4\n",
      "6755 8 7\n",
      "6765 8 6\n",
      "6769 4 9\n",
      "6775 5 8\n",
      "6785 2 4\n",
      "6847 6 4\n",
      "6906 2 6\n",
      "6926 6 4\n",
      "6964 5 3\n",
      "7043 9 7\n",
      "7121 8 9\n",
      "7130 3 9\n",
      "7210 6 8\n",
      "7220 8 3\n",
      "7233 3 1\n",
      "7241 5 4\n",
      "7249 2 4\n",
      "7338 4 8\n",
      "7426 9 4\n",
      "7432 7 1\n",
      "7434 4 8\n",
      "7451 5 4\n",
      "7459 9 5\n",
      "7473 4 8\n",
      "7492 2 7\n",
      "7498 5 3\n",
      "7545 8 9\n",
      "7565 7 4\n",
      "7603 8 5\n",
      "7637 2 3\n",
      "7797 5 6\n",
      "7800 3 2\n",
      "7812 1 8\n",
      "7821 3 2\n",
      "7822 1 2\n",
      "7839 1 8\n",
      "7842 5 8\n",
      "7849 3 2\n",
      "7850 5 6\n",
      "7857 2 4\n",
      "7858 3 2\n",
      "7859 5 4\n",
      "7870 5 4\n",
      "7886 2 9\n",
      "7888 5 4\n",
      "7899 1 8\n",
      "7905 3 2\n",
      "7917 2 4\n",
      "7918 5 8\n",
      "7921 8 1\n",
      "7945 2 6\n",
      "8020 1 8\n",
      "8062 5 8\n",
      "8072 5 3\n",
      "8081 4 6\n",
      "8091 2 8\n",
      "8094 2 8\n",
      "8095 4 6\n",
      "8183 8 5\n",
      "8246 3 5\n",
      "8272 3 5\n",
      "8277 3 5\n",
      "8294 8 5\n",
      "8308 3 5\n",
      "8332 9 7\n",
      "8339 8 6\n",
      "8353 2 4\n",
      "8406 4 9\n",
      "8408 8 4\n",
      "8410 8 6\n",
      "8426 9 4\n",
      "8493 1 8\n",
      "8520 4 9\n",
      "8522 8 6\n",
      "8553 5 3\n",
      "8863 5 6\n",
      "9007 3 8\n",
      "9009 7 2\n",
      "9010 2 8\n",
      "9015 7 2\n",
      "9019 7 2\n",
      "9024 7 2\n",
      "9036 7 2\n",
      "9045 7 2\n",
      "9071 1 8\n",
      "9211 4 9\n",
      "9214 9 4\n",
      "9280 8 5\n",
      "9316 8 9\n",
      "9427 5 3\n",
      "9446 2 4\n",
      "9465 5 3\n",
      "9482 5 3\n",
      "9587 9 4\n",
      "9595 2 5\n",
      "9607 6 5\n",
      "9624 3 8\n",
      "9634 0 3\n",
      "9642 9 7\n",
      "9643 1 7\n",
      "9664 2 7\n",
      "9679 6 5\n",
      "9692 9 7\n",
      "9698 6 5\n",
      "9700 2 1\n",
      "9716 2 0\n",
      "9719 5 0\n",
      "9729 5 6\n",
      "9733 9 8\n",
      "9738 4 6\n",
      "9741 9 7\n",
      "9744 8 1\n",
      "9745 4 2\n",
      "9749 5 6\n",
      "9752 2 0\n",
      "9768 2 0\n",
      "9770 5 0\n",
      "9779 2 0\n",
      "9780 8 1\n",
      "9792 4 9\n",
      "9808 9 4\n",
      "9811 2 8\n",
      "9814 5 8\n",
      "9839 2 7\n",
      "9856 9 5\n",
      "9858 6 8\n",
      "9867 2 8\n",
      "9879 0 2\n",
      "9888 6 0\n",
      "9890 9 4\n",
      "9891 9 4\n",
      "9892 8 6\n",
      "9893 2 8\n",
      "9901 9 4\n",
      "9905 3 7\n",
      "9925 3 2\n",
      "9941 5 6\n",
      "9943 3 8\n",
      "9944 3 9\n",
      "9970 5 3\n",
      "9982 5 6\n",
      "9986 3 8\n",
      "680\n"
     ]
    }
   ],
   "source": [
    "# 틀린거 찾아보기\n",
    "n = 0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y != y_test_pred[i]:\n",
    "        print(i, y, y_test_pred[i])\n",
    "        n += 1\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a5f8fb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "image = np.reshape(X_test[5054], [28, 28])\n",
    "plt.imshow(image, cmap = 'Greys')\n",
    "plt.show()\n",
    "\n",
    "## 학습(이전 학습 내용을 반영하기때문에)을 잘못하면 너무 명백한 예측도 잘 하지못하게된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b8b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd99a659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 64)          51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 1,111,946\n",
      "Trainable params: 1,111,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (5,5), padding = 'valid', activation = 'relu', input_shape = (28, 28, 1)))\n",
    "model.add(layers.MaxPool2D(2, 2))       # 사진 줄이기\n",
    "model.add(layers.Conv2D(64, (5,5), padding = 'valid', activation = 'relu'))\n",
    "model.add(layers.MaxPool2D(2, 2))\n",
    "model.add(layers.Flatten())             #### 2번 ~ 3번이면 충분함\n",
    "model.add(layers.Dense(1024, activation = 'relu'))\n",
    "model.add(layers.Dropout(0.5))                    ### overfit 방지, 일반화된 이미지를 도출\n",
    "model.add(layers.Dense(10, activation = 'relu'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0a5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45927ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f40a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b07c23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
